{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samp3209/capstone/blob/main/Blip_Mass_Output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports and model download\n"
      ],
      "metadata": {
        "id": "yxyYGiXMaBzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2WqpROg-ZrxJ",
        "outputId": "2a93b852-cbfa-4845-a9e5-497728886322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.15.0\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (4.65.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (1.22.4)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (23.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.9/dist-packages (from timm==0.4.12) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm==0.4.12) (0.15.1+cu118)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.4.12) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.4.12) (3.25.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.15.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.15.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.15.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.15.0) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.15.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.15.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.15.0) (1.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.4.12) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Building wheels for collected packages: fairscale, sacremoses\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292853 sha256=c0ff2f397ab3450cc89358e12fe8d26ebdc41e4f98329f0722bb4bea14bfa3a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/88/74/ffb4fe1dd6799cbf410bab623fa7fc27fc3e1973a71a634a9d\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=ddaac9b8cbb108fb0fc7efdfc0baa0244e2e95c8771d761c31420655836c4152\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built fairscale sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, timm, fairscale\n",
            "Successfully installed fairscale-0.4.4 huggingface-hub-0.13.4 sacremoses-0.0.53 timm-0.4.12 tokenizers-0.10.3 transformers-4.15.0\n",
            "Cloning into 'BLIP'...\n",
            "remote: Enumerating objects: 274, done.\u001b[K\n",
            "remote: Total 274 (delta 0), reused 0 (delta 0), pack-reused 274\u001b[K\n",
            "Receiving objects: 100% (274/274), 7.16 MiB | 7.30 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n",
            "/content/BLIP\n"
          ]
        }
      ],
      "source": [
        "#download model\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "    !git clone https://github.com/salesforce/BLIP\n",
        "    %cd BLIP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from models.blip_vqa import blip_vqa\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "OpDyYUuXaAVR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Link google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lPnXQWDHaGHx",
        "outputId": "f6bb471e-14d0-4561-fb4c-9ece97cf01f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path of the folder containing the images\n",
        "folder_path = '/content/drive/MyDrive/MassOutput/Test/'\n",
        "\n",
        "# Create an empty list to store the file paths of the images\n",
        "file_paths = []\n",
        "\n",
        "# Loop through all the files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "  # Check if the file is an image (ends with .jpg, .jpeg, or .png)\n",
        "  if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "    # Get the file path of the image\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    # Append the file path to the list\n",
        "    file_paths.append(file_path)\n",
        "\n",
        "# Print the file paths of the images\n",
        "print(file_paths)"
      ],
      "metadata": {
        "id": "tLhyDXiIaJ5x",
        "outputId": "23018d1c-f37c-4c79-9852-d5f8646c5e0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/MassOutput/Test/<PIL.Image.Image image mode=RGB size=512x512 at 0x7FDB29D6CB80>.png', '/content/drive/MyDrive/MassOutput/Test/<PIL.Image.Image image mode=RGB size=512x512 at 0x7FDB29D7BFD0>.png', '/content/drive/MyDrive/MassOutput/Test/<PIL.Image.Image image mode=RGB size=512x512 at 0x7FDB29D81F10>.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Images and transform\n"
      ],
      "metadata": {
        "id": "eytjs3eRAguN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#opens image from path then stores to list as pil image\n",
        "image_list = []\n",
        "for path in file_paths:\n",
        "  with Image.open(path).convert('RGB') as img:\n",
        "      image_list.append(img)"
      ],
      "metadata": {
        "id": "jNYBzP4yaNWE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for loading images and then transforming them to tensors for captioning\n",
        "def load_images(raw_image):\n",
        "  w,h = raw_image.size\n",
        "  image_size = 512\n",
        "  #display(raw_image.resize((w//5,h//5)))\n",
        "    \n",
        "  transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ]) \n",
        "  image = transform(raw_image).unsqueeze(0).to(device)   \n",
        "  return image"
      ],
      "metadata": {
        "id": "9jwL4KYraROv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transforms inputs and stores them to list \n",
        "inputs = []\n",
        "for image in image_list:\n",
        "    x = load_images(image)\n",
        "    inputs.append(x)"
      ],
      "metadata": {
        "id": "BuW7Sd3oadhf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question the images \n"
      ],
      "metadata": {
        "id": "UWGLWH0aAki4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splits images into promotional and logos and dumps the rest in misc\n",
        "image_size = 512\n",
        "chaunceys = []\n",
        "logos = []\n",
        "misc = []\n",
        "from models.blip import blip_decoder\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "for image in inputs:\n",
        "        question='is subject a beaver?'\n",
        "        question2='is subject a logo with a lake and or mountain?'\n",
        "        with torch.no_grad():\n",
        "          answer = model(image, question, train=False, inference='generate')\n",
        "          answer2 = model(image, question2, train=False, inference='generate')\n",
        "          if answer[0] == 'yes': \n",
        "            chaunceys.append('chauncey the beaver')\n",
        "          elif answer2[0] == 'yes':\n",
        "            logos.append('Champlain College Shield Logo')\n",
        "          else:\n",
        "            misc.append(answer[0])\n"
      ],
      "metadata": {
        "id": "HwbaK8wvafoW",
        "outputId": "eec2fabb-d98f-4675-b241-5008184487ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reshape position embedding from 900 to 1024\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for questioning promotional images\n",
        "q1s = []\n",
        "q2s = []\n",
        "q3s = []\n",
        "q4s = []\n",
        "q5s = []\n",
        "def promotional_questioning(promos):\n",
        "\n",
        "\n",
        "  image_size = 512\n",
        "  model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "  model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "  for image in promos:   \n",
        "      question = 'what is the style of the image?'\n",
        "      question2 = 'what are the colors predominant in the image?'\n",
        "      question3 = 'how is the subject oriented?'\n",
        "      question4 = 'what is the subject doing in this image?'\n",
        "      question5 = 'where does this image take place?'\n",
        "      with torch.no_grad():\n",
        "          answer = model(image, question, train=False, inference='generate') \n",
        "          answer2 = model(image, question2, train=False, inference='generate') \n",
        "          answer3 = model(image, question3, train=False, inference='generate') \n",
        "          answer4 = model(image, question4, train=False, inference='generate') \n",
        "          answer5 = model(image, question5, train=False, inference='generate') \n",
        "          q1s.append(answer[0])\n",
        "          q2s.append(answer2[0])\n",
        "          q3s.append(answer3[0])\n",
        "          q4s.append(answer4[0])\n",
        "          q5s.append(answer5[0])"
      ],
      "metadata": {
        "id": "fKkPqCLMPxnZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for questioning logos\n",
        "cq1s = []\n",
        "cq2s = []\n",
        "cq3s = []\n",
        "cq4s = []\n",
        "cq5s = []\n",
        "def logo_questioning(logos):\n",
        "  from models.blip_vqa import blip_vqa\n",
        "\n",
        "  image_size = 512\n",
        "  model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "  model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "  for image in logos:   \n",
        "      question = 'what is the style of the image?'\n",
        "      question2 = 'what are the colors predominant in the logo?'\n",
        "      question3 = 'what famous artists style does this image image look like?'\n",
        "      question4 = 'what is the shape of the logo?'\n",
        "      question5 = 'what are the predominant features in the logo?'\n",
        "      with torch.no_grad():\n",
        "          answer = model(image, question, train=False, inference='generate') \n",
        "          answer2 = model(image, question2, train=False, inference='generate') \n",
        "          answer3 = model(image, question3, train=False, inference='generate') \n",
        "          answer4 = model(image, question4, train=False, inference='generate') \n",
        "          answer5 = model(image, question5, train=False, inference='generate') \n",
        "\n",
        "          print('answer: '+answer[0])\n",
        "          q1s.append(answer[0])\n",
        "          q2s.append(answer2[0])\n",
        "          q3s.append(answer3[0])\n",
        "          q4s.append(answer4[0])\n",
        "          q5s.append(answer5[0])\n",
        "    "
      ],
      "metadata": {
        "id": "GITnPswtan5v"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test = promotional_questioning(inputs)"
      ],
      "metadata": {
        "id": "riy7A1xfEk26",
        "outputId": "7289edc0-59f6-447f-cd86-067e1cb2c814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reshape position embedding from 900 to 1024\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caption the images and rename in directory\n"
      ],
      "metadata": {
        "id": "t_vxxdXSAwGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chauncey_captions = []\n",
        "for i in range(len(chaunceys)):\n",
        "  nc =  chaunceys[i] + ' ,' + q1s[i] + ' ,' + q2s[i] + ' ,' + q3s[i] + ' ,' + q4s[i] + ' ,' + q5s[i] + '.png'\n",
        "  chauncey_captions.append(nc)\n"
      ],
      "metadata": {
        "id": "IihQpNC8a6ff"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logos_captions = []\n",
        "for i in range(len(logos)):\n",
        "  nc =  logos[i] + ' ,' + cq1s[i] + ' ,' + cq2s[i] + ' ,' + cq3s[i] + ' ,' + cq4s[i] + ' ,' + cq5s[i] + '.png'\n",
        "  logos_captions.append(nc)"
      ],
      "metadata": {
        "id": "bqwW6YEz_xXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#matches the caption to the file path of the images to re name file to caption \n",
        "path = '/content/drive/MyDrive/MassOutput/Test/'\n",
        "def rename_images(new_captions, file_paths):\n",
        "  for i in range(len(file_paths)):\n",
        "    new_name = path + new_captions[i]\n",
        "    old_name = file_paths[i]\n",
        "    os.rename(old_name,new_name)"
      ],
      "metadata": {
        "id": "RD5cUfJva8XH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rename_images(chauncey_captions, file_paths)"
      ],
      "metadata": {
        "id": "QjXCkbUb_UO2",
        "outputId": "992e468a-0220-46dd-a59b-363ae0251e6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c5edc7e3d666>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrename_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_captions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-227399380c6d>\u001b[0m in \u001b[0;36mrename_images\u001b[0;34m(new_captions, file_paths)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnew_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_captions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mold_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/MassOutput/Test/<PIL.Image.Image image mode=RGB size=512x512 at 0x7FDB29D6CB80>.png' -> \"/content/drive/MyDrive/MassOutput/Test/chauncey the beaver ,cartoon ,blue ,like he's holding something ,looking at computer screen ,in front of computer screen.png\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IIsK2LRL_gHP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}