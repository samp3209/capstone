{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnb2nwHxw3Sqj7dP4+9zWf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samp3209/capstone/blob/main/Blip_Mass_Output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports and model download\n"
      ],
      "metadata": {
        "id": "yxyYGiXMaBzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WqpROg-ZrxJ"
      },
      "outputs": [],
      "source": [
        "#download model\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "    !git clone https://github.com/salesforce/BLIP\n",
        "    %cd BLIP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "OpDyYUuXaAVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Link google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lPnXQWDHaGHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path of the folder containing the images\n",
        "folder_path = '/content/drive/MyDrive/MassOutput/Test/'\n",
        "\n",
        "# Create an empty list to store the file paths of the images\n",
        "file_paths = []\n",
        "\n",
        "# Loop through all the files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "  # Check if the file is an image (ends with .jpg, .jpeg, or .png)\n",
        "  if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "    # Get the file path of the image\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    # Append the file path to the list\n",
        "    file_paths.append(file_path)\n",
        "\n",
        "# Print the file paths of the images\n",
        "print(file_paths)"
      ],
      "metadata": {
        "id": "tLhyDXiIaJ5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#opens image from path then stores to list as pil image\n",
        "image_list = []\n",
        "for path in file_paths:\n",
        "  with Image.open(path).convert('RGB') as img:\n",
        "      image_list.append(img)"
      ],
      "metadata": {
        "id": "jNYBzP4yaNWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for loading images and then transforming them to tensors for captioning\n",
        "def load_images(raw_image):\n",
        "  w,h = raw_image.size\n",
        "  image_size = 512\n",
        "  #display(raw_image.resize((w//5,h//5)))\n",
        "    \n",
        "  transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ]) \n",
        "  image = transform(raw_image).unsqueeze(0).to(device)   \n",
        "  return image"
      ],
      "metadata": {
        "id": "9jwL4KYraROv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transforms inputs and stores them to list \n",
        "inputs = []\n",
        "for image in image_list:\n",
        "    x = load_images(image)\n",
        "    inputs.append(x)"
      ],
      "metadata": {
        "id": "BuW7Sd3oadhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#blips through images in input and stores caption to a list\n",
        "captions = []\n",
        "from models.blip import blip_decoder\n",
        "\n",
        "image_size = 512\n",
        "for image in inputs:\n",
        "      model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
        "      model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\n",
        "      model.eval()\n",
        "      model = model.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # beam search\n",
        "          caption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
        "          # nucleus sampling\n",
        "          # caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5) \n",
        "          print('caption: '+caption[0])\n",
        "          captions.append(caption)"
      ],
      "metadata": {
        "id": "HwbaK8wvafoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompts blip with questions about the image \n",
        "q1s = []\n",
        "q2s = []\n",
        "q3s = []\n",
        "q4s = []\n",
        "q5s = []\n",
        "from models.blip_vqa import blip_vqa\n",
        "\n",
        "image_size = 512\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "for image in inputs:   \n",
        "    question = 'what is the style of the image?'\n",
        "    question2 = 'what are the colors predominant in the image?'\n",
        "    question3 = 'what is the mood of the image?'\n",
        "    question4 = 'what is the subject doing in this image?'\n",
        "    question5 = 'where does this image take place?'\n",
        "    with torch.no_grad():\n",
        "        answer = model(image, question, train=False, inference='generate') \n",
        "        answer2 = model(image, question2, train=False, inference='generate') \n",
        "        answer3 = model(image, question3, train=False, inference='generate') \n",
        "        answer4 = model(image, question4, train=False, inference='generate') \n",
        "        answer5 = model(image, question5, train=False, inference='generate') \n",
        "\n",
        "        print('answer: '+answer[0])\n",
        "        q1s.append(answer[0])\n",
        "        q2s.append(answer2[0])\n",
        "        q3s.append(answer3[0])\n",
        "        q4s.append(answer4[0])\n",
        "        q5s.append(answer5[0])"
      ],
      "metadata": {
        "id": "GITnPswtan5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_captions = []\n",
        "for i in range(len(captions)):\n",
        "  nc =  captions[i] + ',' + q1s[i] + ',' + q2s[i] + ',' + q3s[i] + ',' + q4s[i] + ',' + q5s[i] + '.png'\n",
        "  new_captions.append(nc)\n"
      ],
      "metadata": {
        "id": "IihQpNC8a6ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#matches the caption to the file path of the images to re name file to caption\n",
        "path = '/content/drive/MyDrive/MassOutput/Test/'\n",
        "for i in range(len(file_paths)):\n",
        "  new_name = path + new_captions[i]\n",
        "  old_name = file_paths[i]\n",
        "  os.rename(old_name,new_name)"
      ],
      "metadata": {
        "id": "RD5cUfJva8XH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}