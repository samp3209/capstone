{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samp3209/capstone/blob/main/Blip_Mass_Output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports and model download\n"
      ],
      "metadata": {
        "id": "yxyYGiXMaBzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2WqpROg-ZrxJ",
        "outputId": "6545a103-fb9c-4120-a4ab-2e2df3fed761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.15.0\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (1.22.4)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.15.0) (2.27.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm==0.4.12) (0.15.1+cu118)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.9/dist-packages (from timm==0.4.12) (2.0.0+cu118)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.4.12) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.4.12) (3.25.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.15.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.15.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.15.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.15.0) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.15.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.15.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.15.0) (1.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.4.12) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Building wheels for collected packages: fairscale, sacremoses\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292853 sha256=ddfad479a2dd91565165e4412dd2c37c20735b87fd92277757c477f775427913\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/88/74/ffb4fe1dd6799cbf410bab623fa7fc27fc3e1973a71a634a9d\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=96a4762d297071494793a1b2a0d65f7a43f8164b026d72044d43a508d5512734\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built fairscale sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, timm, fairscale\n",
            "Successfully installed fairscale-0.4.4 huggingface-hub-0.13.4 sacremoses-0.0.53 timm-0.4.12 tokenizers-0.10.3 transformers-4.15.0\n",
            "Cloning into 'BLIP'...\n",
            "remote: Enumerating objects: 274, done.\u001b[K\n",
            "remote: Total 274 (delta 0), reused 0 (delta 0), pack-reused 274\u001b[K\n",
            "Receiving objects: 100% (274/274), 7.16 MiB | 8.65 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n",
            "/content/BLIP\n"
          ]
        }
      ],
      "source": [
        "#download model\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "    !git clone https://github.com/salesforce/BLIP\n",
        "    %cd BLIP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "OpDyYUuXaAVR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Link google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lPnXQWDHaGHx",
        "outputId": "2af264f1-ea9a-405a-d5ba-7e96def04cdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path of the folder containing the images\n",
        "folder_path = '/content/drive/MyDrive/MassOutput/Test/'\n",
        "\n",
        "# Create an empty list to store the file paths of the images\n",
        "file_paths = []\n",
        "\n",
        "# Loop through all the files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "  # Check if the file is an image (ends with .jpg, .jpeg, or .png)\n",
        "  if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "    # Get the file path of the image\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    # Append the file path to the list\n",
        "    file_paths.append(file_path)\n",
        "\n",
        "# Print the file paths of the images\n",
        "print(file_paths)"
      ],
      "metadata": {
        "id": "tLhyDXiIaJ5x",
        "outputId": "db016129-e2a8-4dcb-dd00-2735519f7033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/MassOutput/Test/<PIL.Image.Image image mode=RGB size=512x512 at 0x7FDB29D6CB80>.png', '/content/drive/MyDrive/MassOutput/Test/<PIL.Image.Image image mode=RGB size=512x512 at 0x7FDB29D7BFD0>.png', '/content/drive/MyDrive/MassOutput/Test/<PIL.Image.Image image mode=RGB size=512x512 at 0x7FDB29D81F10>.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#opens image from path then stores to list as pil image\n",
        "image_list = []\n",
        "for path in file_paths:\n",
        "  with Image.open(path).convert('RGB') as img:\n",
        "      image_list.append(img)"
      ],
      "metadata": {
        "id": "jNYBzP4yaNWE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for loading images and then transforming them to tensors for captioning\n",
        "def load_images(raw_image):\n",
        "  w,h = raw_image.size\n",
        "  image_size = 512\n",
        "  #display(raw_image.resize((w//5,h//5)))\n",
        "    \n",
        "  transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ]) \n",
        "  image = transform(raw_image).unsqueeze(0).to(device)   \n",
        "  return image"
      ],
      "metadata": {
        "id": "9jwL4KYraROv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transforms inputs and stores them to list \n",
        "inputs = []\n",
        "for image in image_list:\n",
        "    x = load_images(image)\n",
        "    inputs.append(x)"
      ],
      "metadata": {
        "id": "BuW7Sd3oadhf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splits images into promotional and logos and dumps the rest in misc\n",
        "chaunceys = []\n",
        "logos = []\n",
        "misc = []\n",
        "from models.blip import blip_decoder\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "image_size = 512\n",
        "for image in inputs:\n",
        "        question='is subject a beaver?'\n",
        "        question2='is subject a logo with a lake and or mountain?'\n",
        "        with torch.no_grad():\n",
        "          answer = model(image, question, train=False, inference='generate')\n",
        "          answer2 = model(image, question2, train=False, inference='generate')\n",
        "          if answer[0] == 'yes': \n",
        "            chaunceys.append('chauncey the beaver')\n",
        "          elif answer2[0] == 'yes':\n",
        "            logos.append('Champlain College Shield Logo')\n",
        "          else:\n",
        "            misc.append(answer)\n"
      ],
      "metadata": {
        "id": "HwbaK8wvafoW",
        "outputId": "ea8e255a-9c52-4218-d779-1999e9789668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reshape position embedding from 900 to 1024\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for questioning promotional images\n",
        "def promotional_questioning(promos):\n",
        "  q1s = []\n",
        "  q2s = []\n",
        "  q3s = []\n",
        "  q4s = []\n",
        "  q5s = []\n",
        "  from models.blip_vqa import blip_vqa\n",
        "\n",
        "  image_size = 512\n",
        "  model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "  model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "  for image in promos:   \n",
        "      question = 'what is the style of the image?'\n",
        "      question2 = 'what are the colors predominant in the image?'\n",
        "      question3 = 'how is the subject oriented?'\n",
        "      question4 = 'what is the subject doing in this image?'\n",
        "      question5 = 'where does this image take place?'\n",
        "      with torch.no_grad():\n",
        "          answer = model(image, question, train=False, inference='generate') \n",
        "          answer2 = model(image, question2, train=False, inference='generate') \n",
        "          answer3 = model(image, question3, train=False, inference='generate') \n",
        "          answer4 = model(image, question4, train=False, inference='generate') \n",
        "          answer5 = model(image, question5, train=False, inference='generate') \n",
        "          q1s.append(answer[0])\n",
        "          q2s.append(answer2[0])\n",
        "          q3s.append(answer3[0])\n",
        "          q4s.append(answer4[0])\n",
        "          q5s.append(answer5[0])"
      ],
      "metadata": {
        "id": "fKkPqCLMPxnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for questioning logos\n",
        "def logo_questioning(logos):\n",
        "  q1s = []\n",
        "  q2s = []\n",
        "  q3s = []\n",
        "  q4s = []\n",
        "  q5s = []\n",
        "  from models.blip_vqa import blip_vqa\n",
        "\n",
        "  image_size = 512\n",
        "  model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "  model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "  for image in logos:   \n",
        "      question = 'what is the style of the image?'\n",
        "      question2 = 'what are the colors predominant in the logo?'\n",
        "      question3 = 'what famous artists style does this image image look like?'\n",
        "      question4 = 'what is the shape of the logo?'\n",
        "      question5 = 'what are the predominant features in the logo?'\n",
        "      with torch.no_grad():\n",
        "          answer = model(image, question, train=False, inference='generate') \n",
        "          answer2 = model(image, question2, train=False, inference='generate') \n",
        "          answer3 = model(image, question3, train=False, inference='generate') \n",
        "          answer4 = model(image, question4, train=False, inference='generate') \n",
        "          answer5 = model(image, question5, train=False, inference='generate') \n",
        "\n",
        "          print('answer: '+answer[0])\n",
        "          q1s.append(answer[0])\n",
        "          q2s.append(answer2[0])\n",
        "          q3s.append(answer3[0])\n",
        "          q4s.append(answer4[0])\n",
        "          q5s.append(answer5[0])"
      ],
      "metadata": {
        "id": "GITnPswtan5v",
        "outputId": "434eb7a6-3c1a-40d1-9c71-061900112042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reshape position embedding from 900 to 1024\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n",
            "answer: cartoon\n",
            "answer: cartoon\n",
            "answer: cartoon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "riy7A1xfEk26",
        "outputId": "63619393-0ff9-4d6e-a8e9-32fc2428caa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['blue', 'green', 'orange and blue']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_yb4Mg2FOvy",
        "outputId": "8c66f83c-4c1b-4fb1-e489-58bf936370dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yes', 'yes', 'yes']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_captions = []\n",
        "for i in range(len(captions)):\n",
        "  nc =  captions[i] + ',' + q1s[i] + ',' + q2s[i] + ',' + q3s[i] + ',' + q4s[i] + ',' + q5s[i] + '.png'\n",
        "  new_captions.append(nc)\n"
      ],
      "metadata": {
        "id": "IihQpNC8a6ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#matches the caption to the file path of the images to re name file to caption\n",
        "path = '/content/drive/MyDrive/MassOutput/Test/'\n",
        "for i in range(len(file_paths)):\n",
        "  new_name = path + new_captions[i]\n",
        "  old_name = file_paths[i]\n",
        "  os.rename(old_name,new_name)"
      ],
      "metadata": {
        "id": "RD5cUfJva8XH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}